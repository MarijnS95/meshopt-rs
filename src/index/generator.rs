use crate::util::fill_slice;

use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::convert::TryInto;
use std::hash::{BuildHasherDefault, Hash, Hasher};
use std::ops::Range;

const INVALID_INDEX: u32 = u32::MAX;

#[derive(Default)]
struct VertexHasher {
    state: u64,
}

impl Hasher for VertexHasher {
    fn write(&mut self, bytes: &[u8]) {
        // MurmurHash2
        const M: u32 = 0x5bd1e995;
        const R: i32 = 24;

        let mut h: u32 = 0;

        for k4 in bytes.chunks_exact(4) {
            let mut k = u32::from_ne_bytes(k4.try_into().unwrap());

            k = k.wrapping_mul(M);
            k ^= k >> R;
            k = k.wrapping_mul(M);

            h = h.wrapping_mul(M);
            h ^= k;
        }

        self.state = h as u64
    }
    
    fn finish(&self) -> u64 {
        self.state
    }
}

type BuildVertexHasher = BuildHasherDefault<VertexHasher>;

struct VertexWrapper<Vertex> {
    data: Vertex,
    subset: Range<u8>,
}

fn as_bytes<T>(data: &T) -> &[u8] {
    unsafe { std::slice::from_raw_parts(
        (data as *const T) as *const u8,
        std::mem::size_of::<T>(),
    ) }
}

impl<Vertex> PartialEq for VertexWrapper<Vertex> {
    fn eq(&self, other: &Self) -> bool {
        let bytes = as_bytes(&self.data);
        let other_bytes = as_bytes(&other.data);
        let range = Range { start: self.subset.start as usize, end: self.subset.end as usize };
        &bytes[range.clone()] == &other_bytes[range]
    }
}

impl<Vertex> Eq for VertexWrapper<Vertex> {}

impl<Vertex> Hash for VertexWrapper<Vertex> {
    fn hash<H: Hasher>(&self, state: &mut H) {
        let bytes = as_bytes(&self.data);
        let range = Range { start: self.subset.start as usize, end: self.subset.end as usize };
        state.write(&bytes[range]);
    }
}

/// Generates a vertex remap table from the vertex buffer and an optional index buffer and returns number of unique vertices.
///
/// As a result, all vertices that are binary equivalent map to the same (new) location, with no gaps in the resulting sequence.
/// Resulting remap table maps old vertices to new vertices and can be used in [remap_vertex_buffer]/[remap_index_buffer].
///
/// Note that binary equivalence considers all vertex_size bytes, including padding which should be zero-initialized.
///
/// # Arguments
///
/// * `destination`: must contain enough space for the resulting remap table (`vertices.len()` elements)
/// * `indices`: can be `None` if the input is unindexed
pub fn generate_vertex_remap<Vertex>(destination: &mut [u32], indices: Option<&[u32]>, vertices: &[Vertex]) -> usize
where 
    Vertex: Copy
{
    let index_count = match indices {
        Some(buffer) => buffer.len(),
        None => vertices.len(),
    };
    assert_eq!(index_count % 3, 0);
    
    fill_slice(destination, INVALID_INDEX);

    let mut table = HashMap::with_capacity_and_hasher(vertices.len(), BuildVertexHasher::default());

    let mut next_vertex = 0;
    
    let vertex_size: u8 = std::mem::size_of::<Vertex>().try_into().unwrap();

	for i in 0..index_count {
		let index = match indices {
            Some(buffer) => buffer[i] as usize,
            None => i,
        };
		assert!(index < vertices.len());

		if destination[index] == INVALID_INDEX {
            let vertex_wrapper = VertexWrapper { 
                data: vertices[index], 
                subset: (0..vertex_size),
            };

            match table.entry(vertex_wrapper) {
                Entry::Occupied(entry) => {
                    let value = *entry.get() as usize;
                    assert!(destination[value] != INVALID_INDEX);
                    destination[index] = destination[value];
                }
                Entry::Vacant(entry) => {
                    entry.insert(index);
                    destination[index] = next_vertex as u32;
                    next_vertex += 1;
                }
            }
		}
	}

	assert!(next_vertex <= vertices.len());

	next_vertex
}

/// Generates vertex buffer from the source vertex buffer and remap table generated by [generate_vertex_remap].
///
/// # Arguments
///
/// * `destination`: must contain enough space for the resulting vertex buffer (unique_vertex_count elements, returned by [generate_vertex_remap])
/// * `vertex_count`: should be the initial vertex count and not the value returned by [generate_vertex_remap]
pub fn remap_vertex_buffer<Vertex>(destination: &mut [Vertex], vertices: &[Vertex], remap: &[u32]) 
where 
    Vertex: Copy + Eq + Hash
{
    remap
        .iter()
        .filter(|dst| **dst != INVALID_INDEX)
        .enumerate()
        .for_each(|(src, dst)| destination[*dst as usize] = vertices[src]);
}

/// Generates index buffer from the source index buffer and remap table generated by [generate_vertex_remap].
///
/// # Arguments
///
/// * `destination`: must contain enough space for the resulting index buffer (length of `remap` or `indices`)
/// * `indices`: can be `None` if the input is unindexed
pub fn remap_index_buffer(destination: &mut [u32], indices: Option<&[u32]>, remap: &[u32]) {
    let index_count = match indices {
        Some(buffer) => buffer.len(),
        None => remap.len(),
    };
    
    assert_eq!(index_count % 3, 0);

	for i in 0..index_count {
        let index = match indices {
            Some(buffer) => buffer[i] as usize,
            None => i,
        };
        assert!(remap[index] != INVALID_INDEX);

		destination[i] = remap[index];
	}
}

/// Generates index buffer that can be used for more efficient rendering when only a subset of the vertex attributes is necessary.
///
/// All vertices that are binary equivalent (considering `subset` bytes) map to the first vertex in the original vertex buffer.
/// This makes it possible to use the index buffer for Z pre-pass or shadowmap rendering, while using the original index buffer for regular rendering.
///
/// Note that binary equivalence considers all `subset` bytes, including padding which should be zero-initialized.
///
/// # Arguments
///
/// * `destination`: must contain enough space for the resulting index buffer (`indices.len()` elements)
pub fn generate_shadow_index_buffer<Vertex>(destination: &mut [u32], indices: &[u32], vertices: &[Vertex], subset: Range<usize>)
where 
    Vertex: Copy
{
    assert_eq!(indices.len() % 3, 0);
    assert!(subset.end <= u8::MAX as usize);
    
    let mut remap: Vec<u32> = vec![INVALID_INDEX; vertices.len()];

    let mut table = HashMap::with_capacity_and_hasher(vertices.len(), BuildVertexHasher::default());

	for (i, index) in indices.iter().enumerate() {
        let index = *index as usize;
		assert!(index < vertices.len());

		if remap[index] == INVALID_INDEX {
            let vertex_wrapper = VertexWrapper { 
                data: vertices[index], 
                subset: Range { start: subset.start as u8, end: subset.end as u8 },
            };

            remap[index] = match table.entry(vertex_wrapper) {
                Entry::Occupied(entry) => *entry.get(),
                Entry::Vacant(entry) => {
                    entry.insert(index as u32);
                    index as u32
                }
            };
        }
        
        destination[i] = remap[index];
	}
}
